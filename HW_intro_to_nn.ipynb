{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLHTNfSclli"
      },
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtOYB-RHfc_r"
      },
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "KjoeaDrk6fO7"
      },
      "outputs": [],
      "source": [
        "t_inputs = torch.from_numpy(inputs)\n",
        "t_targets = torch.from_numpy(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.]])"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKzbJKfOgGV8"
      },
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "aXhKw6Tdj1-d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x129072510>"
            ]
          },
          "execution_count": 178,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.random.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "eApcB7eb6h9o"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True),\n",
              " tensor([0.6213], requires_grad=True))"
            ]
          },
          "execution_count": 179,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "w, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYGxNGTaf5s6"
      },
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "outputs": [],
      "source": [
        "def model(t_inputs, w, b):\n",
        "    z = t_inputs@torch.t(w) + b\n",
        "    return 1/(1+torch.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "predicted_probs = model(t_inputs, w, b)\n",
        "print(predicted_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "На рандомних значеннях w та b результат кожного рядочка дуже близький до 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      },
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    log1 = true_labels*torch.log(predicted_probs)\n",
        "    log2 = (1-true_labels)*torch.log(1-predicted_probs)\n",
        "    entropies = -(log1 + log2)\n",
        "    return torch.mean(entropies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = binary_cross_entropy(predicted_probs, t_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFKpQxdHi1__"
      },
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[nan, nan, nan]])\n",
            "tensor([nan])\n"
          ]
        }
      ],
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDN1t1RujQsK"
      },
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOPSQyttpVjO"
      },
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "outputs": [],
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "-JwXiSpX6orh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True),\n",
              " tensor([0.0006], requires_grad=True))"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "predicted_probs = model(t_inputs, w, b)\n",
        "print(predicted_probs)\n",
        "loss = binary_cross_entropy(predicted_probs, t_targets)\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ -5.4417, -18.9853, -10.0682]]) tensor([-0.0794])\n"
          ]
        }
      ],
      "source": [
        "print(w.grad, b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCdi44IT334o"
      },
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "outputs": [],
      "source": [
        "def grad_desc(inputs, targets,  model, w, b, alpha, epochs):\n",
        "    epoch = 0\n",
        "    loss_threshold = 0.03\n",
        "    loss_item = float('inf')\n",
        "    while epoch < epochs and loss_item > loss_threshold:\n",
        "        predicted_probs = model(inputs, w, b)\n",
        "        loss = binary_cross_entropy(predicted_probs, targets)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            w -= w.grad * alpha\n",
        "            b -= b.grad * alpha\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "        epoch += 1\n",
        "        loss_item = loss.item()\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch}, loss = {loss.item()}')\n",
        "        \n",
        "    print(f'Final loss:{loss.item()}')\n",
        "    return {'w': w, 'b': b}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100, loss = 0.03297306224703789\n",
            "Epoch 200, loss = 0.03153730183839798\n",
            "Epoch 300, loss = 0.03021698258817196\n",
            "Final loss:0.029990550130605698\n"
          ]
        }
      ],
      "source": [
        "epochs = 1000\n",
        "alpha = 1e-3\n",
        "params = grad_desc(t_inputs, t_targets, model, w, b, alpha, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[-0.4359252154827118, 0.1179201751947403, 0.5006595253944397]]"
            ]
          },
          "execution_count": 238,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params['w'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final values are \n",
            "weights : [[-0.4359252154827118, 0.1179201751947403, 0.5006595253944397]], \n",
            "bias : -0.02110949717462063\n"
          ]
        }
      ],
      "source": [
        "print(f'Final values are \\nweights : {params[\"w\"].tolist()}, \\nbias : {params[\"b\"].item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Запустила градієнтний спуск кілька разів, вважаю, що отримала мінімальне значення loss 0.02 (далі воно провалюється в nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuRhlyF9qAia"
      },
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 453,
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X2dV30KtAPu"
      },
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([73., 67., 43.]), tensor([0.]))\n",
            "(tensor([91., 88., 64.]), tensor([1.]))\n",
            "(tensor([ 87., 134.,  58.]), tensor([1.]))\n"
          ]
        }
      ],
      "source": [
        "t_inputs = torch.from_numpy(inputs)\n",
        "t_targets = torch.from_numpy(targets)\n",
        "train_ds = TensorDataset(t_inputs, t_targets)\n",
        "for i in range(3):\n",
        "    print(train_ds[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nMFaa8suOd3"
      },
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "metadata": {
        "id": "ZCsRo5Mx6wEI"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymcQOo_hum6I"
      },
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "outputs": [],
      "source": [
        "class LogReg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(3, 3)\n",
        "        self.act1 = nn.Sigmoid()\n",
        "        self.linear2 = nn.Linear(3, 1)\n",
        "        self.act2 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        x =  self.linear2(x)\n",
        "        x = self.act2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogReg()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RflV7xeVyoJy"
      },
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), 1e-5)\n",
        "loss_fn = F.binary_cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = model(t_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'targets': t_targets.tolist(), 'predictions': preds.tolist()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 461,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.4531402289867401]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.45314091444015503]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.45312735438346863]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.46413761377334595]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.45313259959220886]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.4531402289867401]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.45314091444015503]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.45312735438346863]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.46413761377334595]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.45313259959220886]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.4531402289867401]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.45314091444015503]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.45312735438346863]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.46413761377334595]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.45313259959220886]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   targets            predictions\n",
              "0    [0.0]   [0.4531402289867401]\n",
              "1    [1.0]  [0.45314091444015503]\n",
              "2    [1.0]  [0.45312735438346863]\n",
              "3    [0.0]  [0.46413761377334595]\n",
              "4    [1.0]  [0.45313259959220886]\n",
              "5    [0.0]   [0.4531402289867401]\n",
              "6    [1.0]  [0.45314091444015503]\n",
              "7    [1.0]  [0.45312735438346863]\n",
              "8    [0.0]  [0.46413761377334595]\n",
              "9    [1.0]  [0.45313259959220886]\n",
              "10   [0.0]   [0.4531402289867401]\n",
              "11   [1.0]  [0.45314091444015503]\n",
              "12   [1.0]  [0.45312735438346863]\n",
              "13   [0.0]  [0.46413761377334595]\n",
              "14   [1.0]  [0.45313259959220886]"
            ]
          },
          "execution_count": 461,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 462,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_data = loss_fn(preds, t_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 463,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7204291820526123\n"
          ]
        }
      ],
      "source": [
        "print(loss_data.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "На мою думку, модель не навчилася"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch-WrYnKzMzq"
      },
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "outputs": [],
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return {'losses': losses, 'model': model}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/1000], Loss: 0.7082\n",
            "Epoch [20/1000], Loss: 0.7081\n",
            "Epoch [30/1000], Loss: 0.7081\n",
            "Epoch [40/1000], Loss: 0.7081\n",
            "Epoch [50/1000], Loss: 0.7081\n",
            "Epoch [60/1000], Loss: 0.7081\n",
            "Epoch [70/1000], Loss: 0.7081\n",
            "Epoch [80/1000], Loss: 0.7080\n",
            "Epoch [90/1000], Loss: 0.7080\n",
            "Epoch [100/1000], Loss: 0.7080\n",
            "Epoch [110/1000], Loss: 0.7080\n",
            "Epoch [120/1000], Loss: 0.7080\n",
            "Epoch [130/1000], Loss: 0.7080\n",
            "Epoch [140/1000], Loss: 0.7079\n",
            "Epoch [150/1000], Loss: 0.7079\n",
            "Epoch [160/1000], Loss: 0.7079\n",
            "Epoch [170/1000], Loss: 0.7079\n",
            "Epoch [180/1000], Loss: 0.7079\n",
            "Epoch [190/1000], Loss: 0.7079\n",
            "Epoch [200/1000], Loss: 0.7079\n",
            "Epoch [210/1000], Loss: 0.7078\n",
            "Epoch [220/1000], Loss: 0.7078\n",
            "Epoch [230/1000], Loss: 0.7078\n",
            "Epoch [240/1000], Loss: 0.7078\n",
            "Epoch [250/1000], Loss: 0.7078\n",
            "Epoch [260/1000], Loss: 0.7078\n",
            "Epoch [270/1000], Loss: 0.7077\n",
            "Epoch [280/1000], Loss: 0.7077\n",
            "Epoch [290/1000], Loss: 0.7077\n",
            "Epoch [300/1000], Loss: 0.7077\n",
            "Epoch [310/1000], Loss: 0.7077\n",
            "Epoch [320/1000], Loss: 0.7077\n",
            "Epoch [330/1000], Loss: 0.7076\n",
            "Epoch [340/1000], Loss: 0.7076\n",
            "Epoch [350/1000], Loss: 0.7076\n",
            "Epoch [360/1000], Loss: 0.7076\n",
            "Epoch [370/1000], Loss: 0.7076\n",
            "Epoch [380/1000], Loss: 0.7076\n",
            "Epoch [390/1000], Loss: 0.7075\n",
            "Epoch [400/1000], Loss: 0.7075\n",
            "Epoch [410/1000], Loss: 0.7075\n",
            "Epoch [420/1000], Loss: 0.7075\n",
            "Epoch [430/1000], Loss: 0.7075\n",
            "Epoch [440/1000], Loss: 0.7075\n",
            "Epoch [450/1000], Loss: 0.7075\n",
            "Epoch [460/1000], Loss: 0.7074\n",
            "Epoch [470/1000], Loss: 0.7074\n",
            "Epoch [480/1000], Loss: 0.7074\n",
            "Epoch [490/1000], Loss: 0.7074\n",
            "Epoch [500/1000], Loss: 0.7074\n",
            "Epoch [510/1000], Loss: 0.7074\n",
            "Epoch [520/1000], Loss: 0.7073\n",
            "Epoch [530/1000], Loss: 0.7073\n",
            "Epoch [540/1000], Loss: 0.7073\n",
            "Epoch [550/1000], Loss: 0.7073\n",
            "Epoch [560/1000], Loss: 0.7073\n",
            "Epoch [570/1000], Loss: 0.7073\n",
            "Epoch [580/1000], Loss: 0.7072\n",
            "Epoch [590/1000], Loss: 0.7072\n",
            "Epoch [600/1000], Loss: 0.7072\n",
            "Epoch [610/1000], Loss: 0.7072\n",
            "Epoch [620/1000], Loss: 0.7072\n",
            "Epoch [630/1000], Loss: 0.7072\n",
            "Epoch [640/1000], Loss: 0.7071\n",
            "Epoch [650/1000], Loss: 0.7071\n",
            "Epoch [660/1000], Loss: 0.7071\n",
            "Epoch [670/1000], Loss: 0.7071\n",
            "Epoch [680/1000], Loss: 0.7071\n",
            "Epoch [690/1000], Loss: 0.7071\n",
            "Epoch [700/1000], Loss: 0.7071\n",
            "Epoch [710/1000], Loss: 0.7070\n",
            "Epoch [720/1000], Loss: 0.7070\n",
            "Epoch [730/1000], Loss: 0.7070\n",
            "Epoch [740/1000], Loss: 0.7070\n",
            "Epoch [750/1000], Loss: 0.7070\n",
            "Epoch [760/1000], Loss: 0.7070\n",
            "Epoch [770/1000], Loss: 0.7069\n",
            "Epoch [780/1000], Loss: 0.7069\n",
            "Epoch [790/1000], Loss: 0.7069\n",
            "Epoch [800/1000], Loss: 0.7069\n",
            "Epoch [810/1000], Loss: 0.7069\n",
            "Epoch [820/1000], Loss: 0.7069\n",
            "Epoch [830/1000], Loss: 0.7069\n",
            "Epoch [840/1000], Loss: 0.7068\n",
            "Epoch [850/1000], Loss: 0.7068\n",
            "Epoch [860/1000], Loss: 0.7068\n",
            "Epoch [870/1000], Loss: 0.7068\n",
            "Epoch [880/1000], Loss: 0.7068\n",
            "Epoch [890/1000], Loss: 0.7068\n",
            "Epoch [900/1000], Loss: 0.7067\n",
            "Epoch [910/1000], Loss: 0.7067\n",
            "Epoch [920/1000], Loss: 0.7067\n",
            "Epoch [930/1000], Loss: 0.7067\n",
            "Epoch [940/1000], Loss: 0.7067\n",
            "Epoch [950/1000], Loss: 0.7067\n",
            "Epoch [960/1000], Loss: 0.7066\n",
            "Epoch [970/1000], Loss: 0.7066\n",
            "Epoch [980/1000], Loss: 0.7066\n",
            "Epoch [990/1000], Loss: 0.7066\n",
            "Epoch [1000/1000], Loss: 0.7066\n"
          ]
        }
      ],
      "source": [
        "loss = fit_return_loss(1000, model, loss_fn, optimizer, train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targets</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.4715062975883484]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.47150564193725586]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.4715048670768738]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.4725654721260071]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.4715052843093872]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.4715062975883484]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.47150564193725586]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.4715048670768738]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.4725654721260071]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.4715052843093872]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.4715062975883484]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.47150564193725586]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.4715048670768738]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[0.0]</td>\n",
              "      <td>[0.4725654721260071]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[1.0]</td>\n",
              "      <td>[0.4715052843093872]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   targets            predictions\n",
              "0    [0.0]   [0.4715062975883484]\n",
              "1    [1.0]  [0.47150564193725586]\n",
              "2    [1.0]   [0.4715048670768738]\n",
              "3    [0.0]   [0.4725654721260071]\n",
              "4    [1.0]   [0.4715052843093872]\n",
              "5    [0.0]   [0.4715062975883484]\n",
              "6    [1.0]  [0.47150564193725586]\n",
              "7    [1.0]   [0.4715048670768738]\n",
              "8    [0.0]   [0.4725654721260071]\n",
              "9    [1.0]   [0.4715052843093872]\n",
              "10   [0.0]   [0.4715062975883484]\n",
              "11   [1.0]  [0.47150564193725586]\n",
              "12   [1.0]   [0.4715048670768738]\n",
              "13   [0.0]   [0.4725654721260071]\n",
              "14   [1.0]   [0.4715052843093872]"
            ]
          },
          "execution_count": 472,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_preds = model(t_inputs)\n",
        "dff = pd.DataFrame({'targets': t_targets.tolist(), 'predictions': final_preds.tolist()})\n",
        "dff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Лосс зменшується, але модель для всіх рядків видає дуже схожі прогнози, що ніяк не дає різницю для класифікації 0 чи 1("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
